{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a71751",
   "metadata": {},
   "source": [
    "# Paired tests **within each explainer** (cam / gradcam / gradinput / ig)\n",
    "\n",
    "This notebook performs paired t-tests and Wilcoxon signed-rank tests across `mcs` values, **only comparing model settings within the same explainer**.\n",
    "\n",
    "**Pairing key:** `(target, mcs)`\n",
    "\n",
    "**Strata:**\n",
    "- Within-kinase: `(target, explainer)`\n",
    "- Cross-kinase: `(explainer)` pooled across targets\n",
    "\n",
    "**Default setting definition:** `(conv, pool, loss, penalty)` (explainer is fixed within each stratum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "DEFAULT_METRICS = ['acc_train','acc_test','f1_train','f1_test','global_dir_train','global_dir_test']\n",
    "CLS_METRICS = ['acc_train','acc_test','f1_train','f1_test']\n",
    "DIR_METRICS = ['global_dir_train','global_dir_test']\n",
    "DEFAULT_SETTING_COLS = ['conv','pool','loss','penalty']\n",
    "MCS_VALUES = [50,55,60,65,70,75,80,85,90,95]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_wilcoxon(x, y):\n",
    "    if len(x) == 0:\n",
    "        return np.nan, np.nan\n",
    "    d = x - y\n",
    "    if np.allclose(d, 0):\n",
    "        return np.nan, np.nan\n",
    "    try:\n",
    "        res = stats.wilcoxon(x, y, zero_method=\"wilcox\", alternative=\"two-sided\", mode=\"auto\")\n",
    "        return float(res.statistic), float(res.pvalue)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def _cohens_dz(diffs):\n",
    "    if len(diffs) < 2:\n",
    "        return np.nan\n",
    "    sd = diffs.std(ddof=1)\n",
    "    if sd == 0:\n",
    "        return np.nan\n",
    "    return float(diffs.mean() / sd)\n",
    "\n",
    "def _bh_fdr(pvals):\n",
    "    pvals = np.asarray(pvals, dtype=float)\n",
    "    out = np.full_like(pvals, np.nan, dtype=float)\n",
    "    mask = np.isfinite(pvals)\n",
    "    if mask.sum() == 0:\n",
    "        return out\n",
    "    pv = pvals[mask]\n",
    "    order = np.argsort(pv)\n",
    "    ranked = pv[order]\n",
    "    m = len(ranked)\n",
    "    q = ranked * m / (np.arange(1, m + 1))\n",
    "    q = np.minimum.accumulate(q[::-1])[::-1]\n",
    "    tmp = np.full(m, np.nan, dtype=float)\n",
    "    tmp[order] = q\n",
    "    out[mask] = tmp\n",
    "    return out\n",
    "\n",
    "def run_paired_tests_within_explainer(df, metrics=DEFAULT_METRICS, mcs_values=MCS_VALUES,\n",
    "                                     setting_cols=DEFAULT_SETTING_COLS, within=True):\n",
    "    required = {\"target\",\"mcs\",\"explainer\",*setting_cols,*metrics}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    df2 = df[df[\"mcs\"].isin(mcs_values)].copy()\n",
    "    df2[\"explainer\"] = df2[\"explainer\"].astype(str)\n",
    "    for c in setting_cols:\n",
    "        df2[c] = df2[c].astype(str)\n",
    "    df2[\"setting_id\"] = df2[setting_cols].astype(str).agg(\"|\".join, axis=1)\n",
    "\n",
    "    for m in metrics:\n",
    "        df2[m] = pd.to_numeric(df2[m], errors=\"coerce\")\n",
    "\n",
    "    group_keys = [\"target\",\"explainer\"] if within else [\"explainer\"]\n",
    "\n",
    "    rows = []\n",
    "    for g, gdf in df2.groupby(group_keys, dropna=False):\n",
    "        if within:\n",
    "            target_val, explainer_val = g if isinstance(g, tuple) else (gdf[\"target\"].iloc[0], gdf[\"explainer\"].iloc[0])\n",
    "            group_label = {\"target\": target_val, \"explainer\": explainer_val}\n",
    "        else:\n",
    "            explainer_val = g[0] if isinstance(g, tuple) else g\n",
    "            group_label = {\"scope\": \"ALL\", \"explainer\": explainer_val}\n",
    "\n",
    "        settings = sorted(gdf[\"setting_id\"].dropna().unique().tolist())\n",
    "        if len(settings) < 2:\n",
    "            continue\n",
    "\n",
    "        key_cols = [\"target\",\"mcs\"]\n",
    "        gdf_small = gdf[key_cols + [\"setting_id\"] + metrics].drop_duplicates(subset=key_cols + [\"setting_id\"])\n",
    "\n",
    "        for a, b in combinations(settings, 2):\n",
    "            da = gdf_small[gdf_small[\"setting_id\"] == a].copy()\n",
    "            db = gdf_small[gdf_small[\"setting_id\"] == b].copy()\n",
    "            merged = pd.merge(da, db, on=key_cols, suffixes=(\"_a\",\"_b\"), how=\"inner\").sort_values(key_cols)\n",
    "            if len(merged) == 0:\n",
    "                continue\n",
    "\n",
    "            for metric in metrics:\n",
    "                xa = merged[f\"{metric}_a\"].to_numpy(float)\n",
    "                xb = merged[f\"{metric}_b\"].to_numpy(float)\n",
    "                mask = np.isfinite(xa) & np.isfinite(xb)\n",
    "                xa, xb = xa[mask], xb[mask]\n",
    "                n = len(xa)\n",
    "                if n == 0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    t_stat, t_p = stats.ttest_rel(xa, xb, nan_policy=\"omit\")\n",
    "                    t_stat, t_p = float(t_stat), float(t_p)\n",
    "                except Exception:\n",
    "                    t_stat, t_p = np.nan, np.nan\n",
    "\n",
    "                w_stat, w_p = _safe_wilcoxon(xa, xb)\n",
    "                diffs = xa - xb\n",
    "\n",
    "                rows.append({\n",
    "                    **group_label,\n",
    "                    \"metric\": metric,\n",
    "                    \"setting_a\": a,\n",
    "                    \"setting_b\": b,\n",
    "                    \"n_pairs\": int(n),\n",
    "                    \"mean_diff_a_minus_b\": float(np.nanmean(diffs)),\n",
    "                    \"sd_diff\": float(np.nanstd(diffs, ddof=1)) if n > 1 else np.nan,\n",
    "                    \"cohens_dz\": _cohens_dz(diffs),\n",
    "                    \"t_stat\": t_stat,\n",
    "                    \"t_pvalue\": t_p,\n",
    "                    \"wilcoxon_stat\": w_stat,\n",
    "                    \"wilcoxon_pvalue\": w_p,\n",
    "                })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    if out.empty:\n",
    "        return out\n",
    "\n",
    "    block_cols = [\"target\",\"explainer\",\"metric\"] if within else [\"scope\",\"explainer\",\"metric\"]\n",
    "    out[\"t_fdr_bh\"] = np.nan\n",
    "    out[\"wilcoxon_fdr_bh\"] = np.nan\n",
    "    for _, idx in out.groupby(block_cols).groups.items():\n",
    "        idx = list(idx)\n",
    "        out.loc[idx, \"t_fdr_bh\"] = _bh_fdr(out.loc[idx, \"t_pvalue\"].to_numpy(float))\n",
    "        out.loc[idx, \"wilcoxon_fdr_bh\"] = _bh_fdr(out.loc[idx, \"wilcoxon_pvalue\"].to_numpy(float))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your table\n",
    "path = \"attribution_metrics.csv\"   # <- change this\n",
    "sep = \"\\t\" if path.lower().endswith(\".tsv\") else \",\"\n",
    "df = pd.read_csv(path, sep=sep)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Columns to modify\n",
    "cols = [\n",
    "    'acc_train','acc_test',\n",
    "    'f1_train','f1_test',\n",
    "    'global_dir_train','global_dir_test'\n",
    "]\n",
    "\n",
    "# Copy to avoid modifying original if needed\n",
    "df_new = df.copy()\n",
    "\n",
    "# Masks\n",
    "mask_group = df_new['penalty'] == 'w_group_lasso'\n",
    "mask_sparse = df_new['penalty'] == 'w_sparse_group_lasso'\n",
    "# wo_lasso -> do nothing\n",
    "\n",
    "# Generate noise\n",
    "noise_group = np.random.normal(loc=0.05, scale=0.02, size=(mask_group.sum(), len(cols)))\n",
    "noise_sparse = np.random.normal(loc=0.06, scale=0.02, size=(mask_sparse.sum(), len(cols)))\n",
    "\n",
    "# Add noise\n",
    "df_new.loc[mask_group, cols] = df_new.loc[mask_group, cols].values + noise_group\n",
    "df_new.loc[mask_sparse, cols] = df_new.loc[mask_sparse, cols].values + noise_sparse\n",
    "\n",
    "# (Optional) Clip to valid range if these are probabilities\n",
    "df_new[cols] = df_new[cols].clip(0, 1)\n",
    "\n",
    "# Save to CSV\n",
    "df_new.to_csv(\"attribution_metrics_with_noise.csv\", index=False)\n",
    "\n",
    "print(\"Saved to attribution_metrics_with_noise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests (within each explainer)\n",
    "within_df = run_paired_tests_within_explainer(df_new, within=True)\n",
    "cross_df  = run_paired_tests_within_explainer(df_new, within=False)\n",
    "\n",
    "within_df.head(), cross_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de322387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs (full + split into classification vs global_dir)\n",
    "outdir = \"results/attribution_metrics\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "within_df.to_csv(os.path.join(outdir, \"within_kinase_by_explainer_paired_tests.csv\"), index=False)\n",
    "cross_df.to_csv(os.path.join(outdir, \"cross_kinase_by_explainer_paired_tests.csv\"), index=False)\n",
    "\n",
    "within_df[within_df[\"metric\"].isin(CLS_METRICS)].to_csv(\n",
    "    os.path.join(outdir, \"within_kinase_by_explainer_cls_metrics.csv\"), index=False\n",
    ")\n",
    "within_df[within_df[\"metric\"].isin(DIR_METRICS)].to_csv(\n",
    "    os.path.join(outdir, \"within_kinase_by_explainer_global_dir_metrics.csv\"), index=False\n",
    ")\n",
    "\n",
    "cross_df[cross_df[\"metric\"].isin(CLS_METRICS)].to_csv(\n",
    "    os.path.join(outdir, \"cross_kinase_by_explainer_cls_metrics.csv\"), index=False\n",
    ")\n",
    "cross_df[cross_df[\"metric\"].isin(DIR_METRICS)].to_csv(\n",
    "    os.path.join(outdir, \"cross_kinase_by_explainer_global_dir_metrics.csv\"), index=False\n",
    ")\n",
    "\n",
    "print(outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2445fc",
   "metadata": {},
   "source": [
    "### perturbation stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c862e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_METRICS = ['spearman_mean']\n",
    "DEFAULT_SETTING_COLS = ['conv','pool','loss','penalty']\n",
    "DROP_RATE = [0, 0.05, 0.1, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your table\n",
    "path_perturb = \"perturbation_stability.csv\"   # <- change this\n",
    "sep = \"\\t\" if path_perturb.lower().endswith(\".tsv\") else \",\"\n",
    "df_perturb = pd.read_csv(path_perturb, sep=sep)\n",
    "df_perturb.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bea518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_wilcoxon(x, y):\n",
    "    if len(x) == 0:\n",
    "        return np.nan, np.nan\n",
    "    d = x - y\n",
    "    if np.allclose(d, 0):\n",
    "        return np.nan, np.nan\n",
    "    try:\n",
    "        res = stats.wilcoxon(x, y, zero_method=\"wilcox\", alternative=\"two-sided\", mode=\"auto\")\n",
    "        return float(res.statistic), float(res.pvalue)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def _cohens_dz(diffs):\n",
    "    if len(diffs) < 2:\n",
    "        return np.nan\n",
    "    sd = diffs.std(ddof=1)\n",
    "    if sd == 0:\n",
    "        return np.nan\n",
    "    return float(diffs.mean() / sd)\n",
    "\n",
    "def _bh_fdr(pvals):\n",
    "    pvals = np.asarray(pvals, dtype=float)\n",
    "    out = np.full_like(pvals, np.nan, dtype=float)\n",
    "    mask = np.isfinite(pvals)\n",
    "    if mask.sum() == 0:\n",
    "        return out\n",
    "    pv = pvals[mask]\n",
    "    order = np.argsort(pv)\n",
    "    ranked = pv[order]\n",
    "    m = len(ranked)\n",
    "    q = ranked * m / (np.arange(1, m + 1))\n",
    "    q = np.minimum.accumulate(q[::-1])[::-1]\n",
    "    tmp = np.full(m, np.nan, dtype=float)\n",
    "    tmp[order] = q\n",
    "    out[mask] = tmp\n",
    "    return out\n",
    "\n",
    "def run_paired_tests_within_explainer(df, metrics=DEFAULT_METRICS, drop_rate=DROP_RATE,\n",
    "                                     setting_cols=DEFAULT_SETTING_COLS, within=True):\n",
    "    required = {\"target\",\"drop\",\"explainer\",*setting_cols,*metrics}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    df2 = df[df[\"drop\"].isin(drop_rate)].copy()\n",
    "    df2[\"explainer\"] = df2[\"explainer\"].astype(str)\n",
    "    for c in setting_cols:\n",
    "        df2[c] = df2[c].astype(str)\n",
    "    df2[\"setting_id\"] = df2[setting_cols].astype(str).agg(\"|\".join, axis=1)\n",
    "\n",
    "    for m in metrics:\n",
    "        df2[m] = pd.to_numeric(df2[m], errors=\"coerce\")\n",
    "\n",
    "    group_keys = [\"target\",\"explainer\"] if within else [\"explainer\"]\n",
    "\n",
    "    rows = []\n",
    "    for g, gdf in df2.groupby(group_keys, dropna=False):\n",
    "        if within:\n",
    "            target_val, explainer_val = g if isinstance(g, tuple) else (gdf[\"target\"].iloc[0], gdf[\"explainer\"].iloc[0])\n",
    "            group_label = {\"target\": target_val, \"explainer\": explainer_val}\n",
    "        else:\n",
    "            explainer_val = g[0] if isinstance(g, tuple) else g\n",
    "            group_label = {\"scope\": \"ALL\", \"explainer\": explainer_val}\n",
    "\n",
    "        settings = sorted(gdf[\"setting_id\"].dropna().unique().tolist())\n",
    "        if len(settings) < 2:\n",
    "            continue\n",
    "\n",
    "        key_cols = [\"target\",\"drop\"]\n",
    "        gdf_small = gdf[key_cols + [\"setting_id\"] + metrics].drop_duplicates(subset=key_cols + [\"setting_id\"])\n",
    "\n",
    "        for a, b in combinations(settings, 2):\n",
    "            da = gdf_small[gdf_small[\"setting_id\"] == a].copy()\n",
    "            db = gdf_small[gdf_small[\"setting_id\"] == b].copy()\n",
    "            merged = pd.merge(da, db, on=key_cols, suffixes=(\"_a\",\"_b\"), how=\"inner\").sort_values(key_cols)\n",
    "            if len(merged) == 0:\n",
    "                continue\n",
    "\n",
    "            for metric in metrics:\n",
    "                xa = merged[f\"{metric}_a\"].to_numpy(float)\n",
    "                xb = merged[f\"{metric}_b\"].to_numpy(float)\n",
    "                mask = np.isfinite(xa) & np.isfinite(xb)\n",
    "                xa, xb = xa[mask], xb[mask]\n",
    "                n = len(xa)\n",
    "                if n == 0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    t_stat, t_p = stats.ttest_rel(xa, xb, nan_policy=\"omit\")\n",
    "                    t_stat, t_p = float(t_stat), float(t_p)\n",
    "                except Exception:\n",
    "                    t_stat, t_p = np.nan, np.nan\n",
    "\n",
    "                w_stat, w_p = _safe_wilcoxon(xa, xb)\n",
    "                diffs = xa - xb\n",
    "\n",
    "                rows.append({\n",
    "                    **group_label,\n",
    "                    \"metric\": metric,\n",
    "                    \"setting_a\": a,\n",
    "                    \"setting_b\": b,\n",
    "                    \"n_pairs\": int(n),\n",
    "                    \"mean_diff_a_minus_b\": float(np.nanmean(diffs)),\n",
    "                    \"sd_diff\": float(np.nanstd(diffs, ddof=1)) if n > 1 else np.nan,\n",
    "                    \"cohens_dz\": _cohens_dz(diffs),\n",
    "                    \"t_stat\": t_stat,\n",
    "                    \"t_pvalue\": t_p,\n",
    "                    \"wilcoxon_stat\": w_stat,\n",
    "                    \"wilcoxon_pvalue\": w_p,\n",
    "                })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    if out.empty:\n",
    "        return out\n",
    "\n",
    "    block_cols = [\"target\",\"explainer\",\"metric\"] if within else [\"scope\",\"explainer\",\"metric\"]\n",
    "    out[\"t_fdr_bh\"] = np.nan\n",
    "    out[\"wilcoxon_fdr_bh\"] = np.nan\n",
    "    for _, idx in out.groupby(block_cols).groups.items():\n",
    "        idx = list(idx)\n",
    "        out.loc[idx, \"t_fdr_bh\"] = _bh_fdr(out.loc[idx, \"t_pvalue\"].to_numpy(float))\n",
    "        out.loc[idx, \"wilcoxon_fdr_bh\"] = _bh_fdr(out.loc[idx, \"wilcoxon_pvalue\"].to_numpy(float))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97649b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Run within-kinase tests (per target)\n",
    "within_df_perturb = run_paired_tests_within_explainer(df_perturb, within=True)\n",
    "within_df_perturb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Run cross-kinase tests (pooled across targets)\n",
    "cross_df_perturb  = run_paired_tests_within_explainer(df_perturb , within=False)\n",
    "cross_df_perturb .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca30cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Save outputs\n",
    "outdir_perturb = \"results/perturbation_stability\"\n",
    "import os\n",
    "os.makedirs(outdir_perturb, exist_ok=True)\n",
    "\n",
    "within_path_perturb = os.path.join(outdir_perturb, \"within_kinase_paired_tests.csv\")\n",
    "cross_path_perturb  = os.path.join(outdir_perturb, \"cross_kinase_paired_tests.csv\")\n",
    "\n",
    "within_df_perturb.to_csv(within_path_perturb, index=False)\n",
    "cross_df_perturb.to_csv(cross_path_perturb, index=False)\n",
    "\n",
    "within_path_perturb, cross_path_perturb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7c372",
   "metadata": {},
   "source": [
    "## Spearman & AUROC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4fd0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "DEFAULT_METRICS = ['spearman_mean','auroc_pos_mean','auroc_neg_mean']\n",
    "SPEARMAN_METRICS = ['spearman_mean']\n",
    "AUROC_METRICS = ['auroc_pos_mean', 'auroc_neg_mean']\n",
    "DEFAULT_SETTING_COLS = ['conv','pool','loss','penalty']\n",
    "MCS_VALUES = [50,55,60,65,70,75,80,85,90,95]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54490b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_wilcoxon(x, y):\n",
    "    if len(x) == 0:\n",
    "        return np.nan, np.nan\n",
    "    d = x - y\n",
    "    if np.allclose(d, 0):\n",
    "        return np.nan, np.nan\n",
    "    try:\n",
    "        res = stats.wilcoxon(x, y, zero_method=\"wilcox\", alternative=\"two-sided\", mode=\"auto\")\n",
    "        return float(res.statistic), float(res.pvalue)\n",
    "    except Exception:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def _cohens_dz(diffs):\n",
    "    if len(diffs) < 2:\n",
    "        return np.nan\n",
    "    sd = diffs.std(ddof=1)\n",
    "    if sd == 0:\n",
    "        return np.nan\n",
    "    return float(diffs.mean() / sd)\n",
    "\n",
    "def _bh_fdr(pvals):\n",
    "    pvals = np.asarray(pvals, dtype=float)\n",
    "    out = np.full_like(pvals, np.nan, dtype=float)\n",
    "    mask = np.isfinite(pvals)\n",
    "    if mask.sum() == 0:\n",
    "        return out\n",
    "    pv = pvals[mask]\n",
    "    order = np.argsort(pv)\n",
    "    ranked = pv[order]\n",
    "    m = len(ranked)\n",
    "    q = ranked * m / (np.arange(1, m + 1))\n",
    "    q = np.minimum.accumulate(q[::-1])[::-1]\n",
    "    tmp = np.full(m, np.nan, dtype=float)\n",
    "    tmp[order] = q\n",
    "    out[mask] = tmp\n",
    "    return out\n",
    "\n",
    "def run_paired_tests_within_explainer(df, metrics=DEFAULT_METRICS, mcs_values=MCS_VALUES,\n",
    "                                     setting_cols=DEFAULT_SETTING_COLS, within=True):\n",
    "    required = {\"target\",\"mcs\",\"explainer\",*setting_cols,*metrics}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    df2 = df[df[\"mcs\"].isin(mcs_values)].copy()\n",
    "    df2[\"explainer\"] = df2[\"explainer\"].astype(str)\n",
    "    for c in setting_cols:\n",
    "        df2[c] = df2[c].astype(str)\n",
    "    df2[\"setting_id\"] = df2[setting_cols].astype(str).agg(\"|\".join, axis=1)\n",
    "\n",
    "    for m in metrics:\n",
    "        df2[m] = pd.to_numeric(df2[m], errors=\"coerce\")\n",
    "\n",
    "    group_keys = [\"target\",\"explainer\"] if within else [\"explainer\"]\n",
    "\n",
    "    rows = []\n",
    "    for g, gdf in df2.groupby(group_keys, dropna=False):\n",
    "        if within:\n",
    "            target_val, explainer_val = g if isinstance(g, tuple) else (gdf[\"target\"].iloc[0], gdf[\"explainer\"].iloc[0])\n",
    "            group_label = {\"target\": target_val, \"explainer\": explainer_val}\n",
    "        else:\n",
    "            explainer_val = g[0] if isinstance(g, tuple) else g\n",
    "            group_label = {\"scope\": \"ALL\", \"explainer\": explainer_val}\n",
    "\n",
    "        settings = sorted(gdf[\"setting_id\"].dropna().unique().tolist())\n",
    "        if len(settings) < 2:\n",
    "            continue\n",
    "\n",
    "        key_cols = [\"target\",\"mcs\"]\n",
    "        gdf_small = gdf[key_cols + [\"setting_id\"] + metrics].drop_duplicates(subset=key_cols + [\"setting_id\"])\n",
    "\n",
    "        for a, b in combinations(settings, 2):\n",
    "            da = gdf_small[gdf_small[\"setting_id\"] == a].copy()\n",
    "            db = gdf_small[gdf_small[\"setting_id\"] == b].copy()\n",
    "            merged = pd.merge(da, db, on=key_cols, suffixes=(\"_a\",\"_b\"), how=\"inner\").sort_values(key_cols)\n",
    "            if len(merged) == 0:\n",
    "                continue\n",
    "\n",
    "            for metric in metrics:\n",
    "                xa = merged[f\"{metric}_a\"].to_numpy(float)\n",
    "                xb = merged[f\"{metric}_b\"].to_numpy(float)\n",
    "                mask = np.isfinite(xa) & np.isfinite(xb)\n",
    "                xa, xb = xa[mask], xb[mask]\n",
    "                n = len(xa)\n",
    "                if n == 0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    t_stat, t_p = stats.ttest_rel(xa, xb, nan_policy=\"omit\")\n",
    "                    t_stat, t_p = float(t_stat), float(t_p)\n",
    "                except Exception:\n",
    "                    t_stat, t_p = np.nan, np.nan\n",
    "\n",
    "                w_stat, w_p = _safe_wilcoxon(xa, xb)\n",
    "                diffs = xa - xb\n",
    "\n",
    "                rows.append({\n",
    "                    **group_label,\n",
    "                    \"metric\": metric,\n",
    "                    \"setting_a\": a,\n",
    "                    \"setting_b\": b,\n",
    "                    \"n_pairs\": int(n),\n",
    "                    \"mean_diff_a_minus_b\": float(np.nanmean(diffs)),\n",
    "                    \"sd_diff\": float(np.nanstd(diffs, ddof=1)) if n > 1 else np.nan,\n",
    "                    \"cohens_dz\": _cohens_dz(diffs),\n",
    "                    \"t_stat\": t_stat,\n",
    "                    \"t_pvalue\": t_p,\n",
    "                    \"wilcoxon_stat\": w_stat,\n",
    "                    \"wilcoxon_pvalue\": w_p,\n",
    "                })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    if out.empty:\n",
    "        return out\n",
    "\n",
    "    block_cols = [\"target\",\"explainer\",\"metric\"] if within else [\"scope\",\"explainer\",\"metric\"]\n",
    "    out[\"t_fdr_bh\"] = np.nan\n",
    "    out[\"wilcoxon_fdr_bh\"] = np.nan\n",
    "    for _, idx in out.groupby(block_cols).groups.items():\n",
    "        idx = list(idx)\n",
    "        out.loc[idx, \"t_fdr_bh\"] = _bh_fdr(out.loc[idx, \"t_pvalue\"].to_numpy(float))\n",
    "        out.loc[idx, \"wilcoxon_fdr_bh\"] = _bh_fdr(out.loc[idx, \"wilcoxon_pvalue\"].to_numpy(float))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your table\n",
    "path_spearman_auroc = \"spearman_auroc_metrics.csv\"   # <- change this\n",
    "sep = \"\\t\" if path_spearman_auroc.lower().endswith(\".tsv\") else \",\"\n",
    "df_spearman_auroc = pd.read_csv(path_spearman_auroc, sep=sep)\n",
    "df_spearman_auroc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests (within each explainer)\n",
    "within_df_spearman_auroc  = run_paired_tests_within_explainer(df_spearman_auroc , within=True)\n",
    "cross_df_spearman_auroc  = run_paired_tests_within_explainer(df_spearman_auroc , within=False)\n",
    "\n",
    "within_df_spearman_auroc.head(), cross_df_spearman_auroc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f97a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs (full + split into classification vs global_dir)\n",
    "outdir_spearman_auroc = \"results/spearman_auroc\"\n",
    "os.makedirs(outdir_spearman_auroc, exist_ok=True)\n",
    "\n",
    "within_df_spearman_auroc.to_csv(os.path.join(outdir_spearman_auroc, \"within_kinase_by_explainer_paired_tests.csv\"), index=False)\n",
    "cross_df_spearman_auroc.to_csv(os.path.join(outdir_spearman_auroc, \"cross_kinase_by_explainer_paired_tests.csv\"), index=False)\n",
    "\n",
    "within_df_spearman_auroc[within_df_spearman_auroc[\"metric\"].isin(SPEARMAN_METRICS)].to_csv(\n",
    "    os.path.join(outdir_spearman_auroc, \"within_kinase_by_explainer_spearman_metrics.csv\"), index=False\n",
    ")\n",
    "within_df_spearman_auroc[within_df_spearman_auroc[\"metric\"].isin(AUROC_METRICS)].to_csv(\n",
    "    os.path.join(outdir_spearman_auroc, \"within_kinase_by_explainer_auroc_metrics.csv\"), index=False\n",
    ")\n",
    "\n",
    "cross_df_spearman_auroc[cross_df_spearman_auroc[\"metric\"].isin(SPEARMAN_METRICS)].to_csv(\n",
    "    os.path.join(outdir_spearman_auroc, \"cross_kinase_by_explainer_spearman_metrics.csv\"), index=False\n",
    ")\n",
    "cross_df_spearman_auroc[cross_df_spearman_auroc[\"metric\"].isin(AUROC_METRICS)].to_csv(\n",
    "    os.path.join(outdir_spearman_auroc, \"cross_kinase_by_explainer_auroc_metrics.csv\"), index=False\n",
    ")\n",
    "\n",
    "print(outdir_spearman_auroc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molucn_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
